{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import seaborn as sns\n",
    "import custom_module as cm\n",
    "#import optimizer_module as om"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "####################### DATA PREPROCESSING #################################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "data = pd.read_csv(\"combined_data.csv\", parse_dates=True)\n",
    "#data = data[\"2016\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load this file for saving time\n",
    "# Saving the data file so we can reload with the features made again to reduce time\n",
    "# data.to_csv('final.csv') \n",
    "data = pd.read_csv(\"final.csv\", parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RRP5MIN</th>\n",
       "      <th>RESIDUAL_DEMAND</th>\n",
       "      <th>AVG_PRICE</th>\n",
       "      <th>DIFF_PRICE</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>business hour</th>\n",
       "      <th>public holiday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SETTLEMENTDATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>4.096091</td>\n",
       "      <td>1593.93</td>\n",
       "      <td>5.046193</td>\n",
       "      <td>-0.486409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:05:00</th>\n",
       "      <td>5.257498</td>\n",
       "      <td>1557.06</td>\n",
       "      <td>5.102443</td>\n",
       "      <td>1.161408</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:10:00</th>\n",
       "      <td>5.415838</td>\n",
       "      <td>1510.10</td>\n",
       "      <td>5.087579</td>\n",
       "      <td>0.158340</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:15:00</th>\n",
       "      <td>5.415838</td>\n",
       "      <td>1474.70</td>\n",
       "      <td>5.087579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:20:00</th>\n",
       "      <td>3.609315</td>\n",
       "      <td>1464.90</td>\n",
       "      <td>4.937838</td>\n",
       "      <td>-1.806523</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30 23:40:00</th>\n",
       "      <td>5.998145</td>\n",
       "      <td>546.95</td>\n",
       "      <td>4.556324</td>\n",
       "      <td>-0.136612</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30 23:45:00</th>\n",
       "      <td>5.961220</td>\n",
       "      <td>560.19</td>\n",
       "      <td>4.778101</td>\n",
       "      <td>-0.036925</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30 23:50:00</th>\n",
       "      <td>5.066510</td>\n",
       "      <td>559.87</td>\n",
       "      <td>4.907920</td>\n",
       "      <td>-0.894710</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30 23:55:00</th>\n",
       "      <td>5.002401</td>\n",
       "      <td>547.07</td>\n",
       "      <td>5.023500</td>\n",
       "      <td>-0.064109</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01 00:00:00</th>\n",
       "      <td>5.063096</td>\n",
       "      <td>570.31</td>\n",
       "      <td>5.158584</td>\n",
       "      <td>0.060695</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367777 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      RRP5MIN  RESIDUAL_DEMAND  AVG_PRICE  DIFF_PRICE  hour  \\\n",
       "SETTLEMENTDATE                                                                \n",
       "2016-01-01 00:00:00  4.096091          1593.93   5.046193   -0.486409     0   \n",
       "2016-01-01 00:05:00  5.257498          1557.06   5.102443    1.161408     0   \n",
       "2016-01-01 00:10:00  5.415838          1510.10   5.087579    0.158340     0   \n",
       "2016-01-01 00:15:00  5.415838          1474.70   5.087579    0.000000     0   \n",
       "2016-01-01 00:20:00  3.609315          1464.90   4.937838   -1.806523     0   \n",
       "...                       ...              ...        ...         ...   ...   \n",
       "2019-06-30 23:40:00  5.998145           546.95   4.556324   -0.136612    23   \n",
       "2019-06-30 23:45:00  5.961220           560.19   4.778101   -0.036925    23   \n",
       "2019-06-30 23:50:00  5.066510           559.87   4.907920   -0.894710    23   \n",
       "2019-06-30 23:55:00  5.002401           547.07   5.023500   -0.064109    23   \n",
       "2019-07-01 00:00:00  5.063096           570.31   5.158584    0.060695     0   \n",
       "\n",
       "                     weekday  month  business hour  public holiday  \n",
       "SETTLEMENTDATE                                                      \n",
       "2016-01-01 00:00:00        0      1              0               1  \n",
       "2016-01-01 00:05:00        0      1              0               1  \n",
       "2016-01-01 00:10:00        0      1              0               1  \n",
       "2016-01-01 00:15:00        0      1              0               1  \n",
       "2016-01-01 00:20:00        0      1              0               1  \n",
       "...                      ...    ...            ...             ...  \n",
       "2019-06-30 23:40:00        0      6              0               0  \n",
       "2019-06-30 23:45:00        0      6              0               0  \n",
       "2019-06-30 23:50:00        0      6              0               0  \n",
       "2019-06-30 23:55:00        0      6              0               0  \n",
       "2019-07-01 00:00:00        0      7              0               0  \n",
       "\n",
       "[367777 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SETTLEMENTDATE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SETTLEMENTDATE'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ea3b3e8a8bf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Setting the data to index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SETTLEMENTDATE\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SETTLEMENTDATE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SETTLEMENTDATE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"SETTLEMENTDATE\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SETTLEMENTDATE'"
     ]
    }
   ],
   "source": [
    "# Setting the data to index\n",
    "data[\"SETTLEMENTDATE\"] = pd.to_datetime(data[\"SETTLEMENTDATE\"])\n",
    "data.index = data[\"SETTLEMENTDATE\"]\n",
    "data.drop(columns=\"SETTLEMENTDATE\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace outliers by outlier threshold\n",
    "data = cm.replace_outliers(data, 'RRP5MIN', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the test dataset for testing purposes in evaluation\n",
    "X_test = data[\"2019-01-01\":\"2019-06-30\"].copy()\n",
    "X_test = X_test[\"RRP5MIN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### SOME CHARTS OF THE DATA IN HAND ####################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curve Price\n",
    "cm.plot_chart(data[\"RRP5MIN\"].loc[\"2018-02-20\":\"2018-02-21\"], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curve Residual Demand\n",
    "# Looks stationary\n",
    "cm.plot_chart(data[\"RESIDUAL_DEMAND\"].loc[\"2019\"], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MORE IF YOU WANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MORE IF YOU WANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "####################### FEATURES PREPROCESSING #############################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg Price of last 1 hour i.e. 12 data points at 5 minutes granularity\n",
    "data[\"AVG_PRICE\"] = pd.DataFrame(cm.average_hours(data[\"RRP5MIN\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differencing the average price and creating a differenced price variable\n",
    "data[\"AVG_PRICE\"] = cm.period_difference(data[\"AVG_PRICE\"])\n",
    "data[\"DIFF_PRICE\"] = cm.period_difference(data[\"RRP5MIN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'hour', 'weekday' and 'month' features\n",
    "data['hour'] = 0\n",
    "data['weekday'] = 0\n",
    "data['month'] = 0\n",
    "for i in range(len(data)):\n",
    "    position = data.index[i]\n",
    "    data['hour'][i] = position.hour\n",
    "    data['weekday'][i] = position.weekday()\n",
    "    data['month'][i] = position.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING FEATURES\n",
    "# Generate 'business hour' feature. 7am-7pm business hours\n",
    "data[\"business hour\"] = 0\n",
    "for i in range(len(data)):\n",
    "    position = data.index[i]\n",
    "    hour = position.hour\n",
    "    if ((hour > 7 and hour < 12) or (hour > 14 and hour < 19)):\n",
    "        data[\"business hour\"][i] = 2\n",
    "    elif (hour >= 12 and hour <= 14):\n",
    "        data[\"business hour\"][i] = 1\n",
    "    else:\n",
    "        data[\"business hour\"][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'weekend' feature\n",
    "for i in range(len(data)):\n",
    "    position = data.index[i]\n",
    "    weekday = position.weekday()\n",
    "    if (weekday == 6):\n",
    "        data['weekday'][i] = 2\n",
    "    elif (weekday == 5):\n",
    "        data['weekday'][i] = 1\n",
    "    else:\n",
    "        data['weekday'][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "#pip install holidays\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus_holidays = holidays.CountryHoliday('AUS', prov='NSW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"public holiday\"] = 0\n",
    "for i in range(len(data)):\n",
    "    if (data.index[i] in aus_holidays):\n",
    "        data[\"public holiday\"][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SAVE FILE HERE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "########### PREPARING DATA FOR KERAS TO PROCESS PREPROCESSING ##############\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## 2 MODELS #######################################################\n",
    "######### 1st for processing Categorical Data for Regression via Multi-Layer Perceptron #########\n",
    "########################### 2nd for processing Time Series via LSTM ##############################\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the RRP between 0 and 1 as required by the NN\n",
    "features = ['RESIDUAL_DEMAND', 'AVG_PRICE', 'DIFF_PRICE']\n",
    "feature_scaler = MinMaxScaler()\n",
    "for i in features:\n",
    "    data[i] = feature_scaler.fit_transform(pd.DataFrame(data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale price data to 0-1 range\n",
    "label_scaler = MinMaxScaler()\n",
    "data['RRP5MIN'] = label_scaler.fit_transform(data['RRP5MIN'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data['2016-12-25 00:00:00':].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include time lags of timeseries data for last day i.e. 288 data points at 5 minutes granularity\n",
    "# Also 80 lags of same day previous week\n",
    "\n",
    "# Creating Daily lags\n",
    "for i in range(1,201):\n",
    "    train[\"price_l_{}\".format(i)] = train[\"DIFF_PRICE\"].shift(i)\n",
    "    train[\"demand_l_{}\".format(i)] = train[\"RESIDUAL_DEMAND\"].shift(i)\n",
    "    train[\"avgPrice_l_{}\".format(i)] = train[\"AVG_PRICE\"].shift(i)\n",
    "    \n",
    "\n",
    "# Creating Week ago lags\n",
    "j = 1\n",
    "size = 2016\n",
    "for i in range(size, size-80, -1):\n",
    "    train[\"w_price_l_{}\".format(j)] = train[\"DIFF_PRICE\"].shift(i)\n",
    "    train[\"w_demand_l_{}\".format(j)] = train[\"RESIDUAL_DEMAND\"].shift(i)\n",
    "    train[\"w_avgPrice_l_{}\".format(j)] = train[\"AVG_PRICE\"].shift(i)\n",
    "    j+=1\n",
    "\n",
    "\n",
    "# # Creating Month ago lags\n",
    "# j = 1\n",
    "# for i in range(1728,2016):\n",
    "#     train[\"w_l_{}\".format(j)] = train[\"RRP5MIN\"].shift(i)\n",
    "#     j+=1\n",
    "        \n",
    "#Adjustment for leap year required Here!!!!!!!!!!\n",
    "#Creating year ago lags\n",
    "# j = 1\n",
    "# size = data['2017'].shape[0]\n",
    "# for i in range(size, size-50, -1):\n",
    "#     train[\"y_price_l_{}\".format(j)] = train[\"DIFF_PRICE\"].shift(i)\n",
    "#     train[\"y_demand_l_{}\".format(j)] = train[\"RESIDUAL_DEMAND\"].shift(i)\n",
    "#     train[\"y_avgPrice_l_{}\".format(j)] = train[\"AVG_PRICE\"].shift(i)\n",
    "#     j+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NANS\n",
    "train.dropna(inplace=True)\n",
    "train.head(5)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### PROCESSING THE DATA FOR MLP NETWORK ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### THIS IS FOR MULTILAYER PERCEPTRON PURPOSES\n",
    "train1 = data[['hour', 'weekday', 'month', 'business hour', 'public holiday', 'RRP5MIN']]\n",
    "train1 = train1[\"2017\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the categorical variables using the same scaler used for LSTM variables\n",
    "cont = ['hour', 'weekday', 'month', 'business hour', 'public holiday']\n",
    "for i in cont:\n",
    "    train1[i] = feature_scaler.transform(pd.DataFrame(train1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = train1[train1.index.minute == 0]\n",
    "features1 = features1[features1.index.hour == 0]\n",
    "\n",
    "# Seperating training and test data for Multi-Layer Perceptron Network\n",
    "features_train1 = features1[:'2018']\n",
    "features_test1 = features1['2019':'2019-06-30']\n",
    "\n",
    "# Reshaping the features and test data to NP-Array as per Keras input requirement\n",
    "features_train1 = features_train1.to_numpy().reshape(features_train1.shape[0], features_train1.shape[1])\n",
    "features_test1 = features_test1.to_numpy().reshape(features_test1.shape[0], features_test1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### PROCESSING THE DATA FOR LSTM NETWORK ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature and label dataframes\n",
    "prelim_features = train.drop(['RRP5MIN', 'RESIDUAL_DEMAND', 'AVG_PRICE', 'DIFF_PRICE', 'hour', 'weekday', 'month', 'business hour', 'public holiday'], axis=1)\n",
    "prelim_labels = pd.DataFrame(train[['RRP5MIN']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format labels to 24 hour output range\n",
    "for i in range(0, 288):\n",
    "    prelim_labels['t_{}'.format(i)] = prelim_labels['RRP5MIN'].shift(-i)\n",
    "prelim_labels.drop(['RRP5MIN'], axis=1, inplace=True)\n",
    "\n",
    "# apply one-day discretization to the data\n",
    "labels = prelim_labels[prelim_labels.index.minute == 0]\n",
    "labels = labels[labels.index.hour == 0]\n",
    "features = prelim_features[prelim_features.index.minute == 0]\n",
    "features = features[features.index.hour == 0]\n",
    "\n",
    "features_train = features[:'2018']\n",
    "features_test = features['2019':'2019-06-30']\n",
    "labels_train = labels[:'2018']\n",
    "\n",
    "samples_train = len(features_train)\n",
    "samples_test = len(features_test)\n",
    "timesteps = 280\n",
    "\n",
    "# convert pandas data frames to numpy ndarrays\n",
    "features_train = features_train.to_numpy().reshape(samples_train, timesteps, 3)\n",
    "features_test = features_test.to_numpy().reshape(samples_test, timesteps, 3)\n",
    "labels_train = labels_train.to_numpy()\n",
    "\n",
    "# check for correct data shape\n",
    "features_train.shape, labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import json\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and validation data\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(features_train, labels_train, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model.compile(loss='mae', optimizer='adam')\n",
    "# checkpoint = ModelCheckpoint('./models/multidim_timeseries_testing.hdf5', save_best_only=True)\n",
    "\n",
    "# hist = rnn.fit(X_train, y_train,\n",
    "#                  validation_data=(X_valid, y_valid),\n",
    "#                  callbacks=[checkpoint], \n",
    "#                  verbose=1, batch_size=16, epochs=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn = cm.RNN_model(64, 64, 128, input_s=(X_train.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='mae', optimizer='adam')\n",
    "# # checkpoint = ModelCheckpoint('./models/multidim_timeseries_testing.hdf5', save_best_only=True)\n",
    "\n",
    "# hist = model.fit(X_train, y_train,\n",
    "#                  validation_data=(X_valid, y_valid),\n",
    "#                  callbacks=[checkpoint], \n",
    "#                  verbose=1, batch_size=50, epochs=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # results, hist = cm.train_predict_evaluate(rnn, X_train, X_valid, y_train, y_valid, features_test,\n",
    "#                                        X_test.to_numpy().flatten(), X_test.index, label_scaler, 32, 160, \n",
    "#                                        'multidim_timeseries_testing.hdf5', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best = load_model('./models/multidim_timeseries_testing.hdf5')\n",
    "# # pred = best.predict([input_test, input_test[:, :, 3]])\n",
    "# pred = best.predict(features_test)\n",
    "# #pred = scaler.inverse_transform(pred.flatten().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "########### CONCATENATE THE 2 NN & COMPILE THEM TO FORM BIGGER NN ##########\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the 2 models\n",
    "mlp = cm.create_mlp((features_train1.shape[1],))\n",
    "lstm = cm.create_conv_lstm((None, features_train.shape[1], 3))                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the 2 networks into a bigger network \n",
    "combinedInput = concatenate([mlp.output, lstm.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the bigger Network to the output layer to predict one-day ahead i.e. 288 intervals\n",
    "x = Dense(32, activation=\"relu\")(combinedInput)\n",
    "x = Dense(288, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=[mlp.input, lstm.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model with Mean Absolute Error as loss function and Adam as optimizer\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "#checkpoint = ModelCheckpoint('./models/multidim_timeseries_testing.hdf5', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "hist = model.fit(x=[features_train1, features_train], y=labels_train, \n",
    "                 verbose=1, batch_size=50, epochs=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions on the Testing Data\n",
    "pred = model.predict([features_test1, features_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse scaling the predictions and re-shaping it to 1D output vector\n",
    "pred = label_scaler.inverse_transform(pred.flatten().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Predictions and True Values in results dataframe\n",
    "results = pd.DataFrame({'prediction':pred.flatten(), 'true values':X_test}, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of predictions against Actuals\n",
    "cm.plot_chart(results[\"2019-01-01\":\"2019-01-10\"], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss comparision plot\n",
    "cm.plot_chart(pd.DataFrame(hist.history), xlab='Training Epoch', ylab='Mean Squared Error', title='Training and Validation Error over the Course of Training', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantifying Performance using MAE, MSE, RMSE\n",
    "cm.quantify_performance(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CHECKING MODELS ##########\n",
    "models_list = []\n",
    "models_list.append(cm.RNN_model(64, 64, 128, input_s=(X_train.shape[1],1)))\n",
    "models_list.append(cm.RNN_model(128, 32, 64, input_s=(X_train.shape[1],1)))\n",
    "models_list.append(cm.RNN_model(64, 32, 64, input_s=(X_train.shape[1],1)))\n",
    "models_list.append(cm.RNN_model(32, 16, 32, input_s=(X_train.shape[1],1)))\n",
    "models_list.append(cm.RNN_model(128, 32, 64, input_s=(X_train.shape[1],1)))\n",
    "models_list.append(cm.RNN_model(128, 64, 128, input_s=(X_train.shape[1],1)))\n",
    "# train all archtitectures and evaluate performance on the test set\n",
    "for i, rnn in enumerate(models_list):\n",
    "\n",
    "    results, hist = cm.train_predict_evaluate(rnn, X_train, X_valid, y_train, y_valid, features_test,\n",
    "                                       X_test.to_numpy().flatten(), X_test.index, label_scaler, 32, 160, \n",
    "                                       'timeseries_architecture_tests.hdf5', verbose=0)\n",
    "    print(\"Model {}\".format(i))\n",
    "    cm.quantify_performance(results)\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = train_test_split(features_train, features_train1, test_size=0.25, random_state=42)\n",
    "# (features_training, features_testing, features_training1, features_testing1) = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### SEPERATING Y TRAIN AND Y TEST FOR VALIDATION PURPOSES\n",
    "# trainY = features_training1[\"RRP5MIN\"]\n",
    "# testY = features_testing1[\"RRP5MIN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_training1.drop(columns=[\"RRP5MIN\"], inplace=True)\n",
    "# features_testing1.drop(columns=[\"RRP5MIN\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_training1 = features_training1.to_numpy().reshape(features_training1.shape[0], features_training1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_testing1 = features_testing1.to_numpy().reshape(features_testing1.shape[0], features_testing1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Hour of the day\n",
    "# Day of the week\n",
    "# Flag of Business Hour\n",
    "# Public Holiday Flag\n",
    "# Forecast Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"2019\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input prices into optimizer\n",
    "import optimizer_module as om\n",
    "numDays = 100 # Number of days to run model\n",
    "start = 0 # Starting time interval from price data\n",
    "bStorage0 = 0 # Starting battery charge\n",
    "\n",
    "predPrices = results.iloc[start:start+(numDays*288)][\"prediction\"].tolist()\n",
    "realPrices = results.iloc[start:start+(numDays*288)][\"true values\"].tolist()\n",
    "outputResults = 1\n",
    "outputActions = 1\n",
    "\n",
    "print(\"Optimizer results for real prices\")\n",
    "realNxtAction, realNxtBatCharge, realActions = om.optimize(realPrices, bStorage0, outputResults, outputActions)\n",
    "\n",
    "print(\"\\nOptimizer results for predicted prices\\nProfit is incorrect as it is calculating predicted profit not actual profit\")\n",
    "predNxtAction, predNxtBatCharge, predActions = om.optimize(predPrices, bStorage0, outputResults, outputActions)\n",
    "\n",
    "maxProfit = sum([realActions[i]*realPrices[i]/12 for i in range(numDays*288)])\n",
    "actualProfit = sum([predActions[i]*realPrices[i]/12 for i in range(numDays*288)])\n",
    "\n",
    "print(\"\\n----------RESULTS----------\")\n",
    "print(\"Max profit possible: $%.4g\" % (maxProfit))\n",
    "print(\"Actual profit: $%.4g -> %.4g%% of max profit possible\" % (actualProfit,actualProfit/maxProfit*100))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
