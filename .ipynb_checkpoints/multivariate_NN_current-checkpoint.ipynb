{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import seaborn as sns\n",
    "import custom_module as cm\n",
    "#import optimizer_module as om"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "####################### DATA PREPROCESSING #################################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot do slice indexing on RangeIndex with these indexers [2016] of type str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6ebe2e5e6fff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Loading the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"combined_data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"2016\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2880\u001b[0m         \u001b[1;31m# Do we have a slicer (on rows)?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2881\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_index_sliceable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2882\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2883\u001b[0m             \u001b[1;31m# either we have a slice or we have a string that can be converted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36mconvert_to_index_sliceable\u001b[1;34m(obj, key)\u001b[0m\n\u001b[0;32m   2132\u001b[0m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2134\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_slice_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2136\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_convert_slice_indexer\u001b[1;34m(self, key, kind)\u001b[0m\n\u001b[0;32m   3154\u001b[0m             \"\"\"\n\u001b[0;32m   3155\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_index_slice\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3156\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"slice\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3157\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"slice\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3158\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"slice\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_validate_indexer\u001b[1;34m(self, form, key, kind)\u001b[0m\n\u001b[0;32m   4995\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4996\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4997\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalid_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4999\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_maybe_cast_slice_bound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_invalid_indexer\u001b[1;34m(self, form, key)\u001b[0m\n\u001b[0;32m   3266\u001b[0m         \"\"\"\n\u001b[0;32m   3267\u001b[0m         raise TypeError(\n\u001b[1;32m-> 3268\u001b[1;33m             \u001b[1;34mf\"cannot do {form} indexing on {type(self).__name__} with these \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3269\u001b[0m             \u001b[1;34mf\"indexers [{key}] of type {type(key).__name__}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3270\u001b[0m         )\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot do slice indexing on RangeIndex with these indexers [2016] of type str"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "data = pd.read_csv(\"combined_data.csv\", parse_dates=True)\n",
    "data = data[\"2016\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load this file for saving time\n",
    "# Saving the data file so we can reload with the features made again to reduce time\n",
    "# data.to_csv('final.csv') \n",
    "data = pd.read_csv(\"final.csv\", parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the data to index\n",
    "data[\"SETTLEMENTDATE\"] = pd.to_datetime(data[\"SETTLEMENTDATE\"])\n",
    "data.index = data[\"SETTLEMENTDATE\"]\n",
    "data.drop(columns=\"SETTLEMENTDATE\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace outliers by outlier threshold\n",
    "data = cm.replace_outliers(data, 'RRP5MIN', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the test dataset for testing purposes in evaluation\n",
    "X_test = data[\"2019-01-01\":\"2019-06-30\"].copy()\n",
    "X_test = X_test[\"RRP5MIN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### SOME CHARTS OF THE DATA IN HAND ####################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curve Price\n",
    "cm.plot_chart(data[\"RRP5MIN\"].loc[\"2018-02-20\":\"2018-02-21\"], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curve Residual Demand\n",
    "# Looks stationary\n",
    "cm.plot_chart(data[\"RESIDUAL_DEMAND\"].loc[\"2019\"], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether the series is Stationary via Dickey Fuller Test\n",
    "# TESTING FOR PRICE\n",
    "cm.is_Stationary(data.loc[\"2018\"][\"RRP5MIN\"])\n",
    "# TESTING FOR RESIDUAL DEMAND\n",
    "cm.is_Stationary(data.loc[\"2018\"][\"RESIDUAL_DEMAND\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function plots the ACF and PACF plots to find useful lags for time series\n",
    "cm.serial_corr(data.loc[\"2018\"]['RRP5MIN'], lags=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ADD CORRELATION PLOT HERE WITH OTHER VARIABLES #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MORE IF YOU WANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "####################### FEATURES PREPROCESSING #############################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg Price of last 1 hour i.e. 12 data points at 5 minutes granularity\n",
    "data[\"AVG_PRICE\"] = pd.DataFrame(cm.average_hours(data[\"RRP5MIN\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differencing the average price and creating a differenced price variable\n",
    "data[\"AVG_PRICE\"] = cm.period_difference(data[\"AVG_PRICE\"])\n",
    "data[\"DIFF_PRICE\"] = cm.period_difference(data[\"RRP5MIN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'hour', 'weekday' and 'month' features\n",
    "data['hour'] = 0\n",
    "data['weekday'] = 0\n",
    "data['month'] = 0\n",
    "for i in range(len(data)):\n",
    "    position = data.index[i]\n",
    "    data['hour'][i] = position.hour\n",
    "    data['weekday'][i] = position.weekday()\n",
    "    data['month'][i] = position.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING FEATURES\n",
    "# Generate 'business hour' feature. 7am-7pm business hours\n",
    "data[\"business hour\"] = 0\n",
    "for i in range(len(data)):\n",
    "    position = data.index[i]\n",
    "    hour = position.hour\n",
    "    if ((hour > 7 and hour < 12) or (hour > 14 and hour < 19)):\n",
    "        data[\"business hour\"][i] = 2\n",
    "    elif (hour >= 12 and hour <= 14):\n",
    "        data[\"business hour\"][i] = 1\n",
    "    else:\n",
    "        data[\"business hour\"][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'weekend' feature\n",
    "for i in range(len(data)):\n",
    "    position = data.index[i]\n",
    "    weekday = position.weekday()\n",
    "    if (weekday == 6):\n",
    "        data['weekday'][i] = 2\n",
    "    elif (weekday == 5):\n",
    "        data['weekday'][i] = 1\n",
    "    else:\n",
    "        data['weekday'][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus_holidays = holidays.CountryHoliday('AUS', prov='SA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"public holiday\"] = 0\n",
    "for i in range(len(data)):\n",
    "    if (data.index[i] in aus_holidays):\n",
    "        data[\"public holiday\"][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SAVE FILE HERE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "########### PREPARING DATA FOR KERAS TO PROCESS PREPROCESSING ##############\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## 2 MODELS #######################################################\n",
    "######### 1st for processing Categorical Data for Regression via Multi-Layer Perceptron #########\n",
    "########################### 2nd for processing Time Series via LSTM ##############################\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the RRP between 0 and 1 as required by the NN\n",
    "features = ['RESIDUAL_DEMAND', 'AVG_PRICE', 'DIFF_PRICE']\n",
    "feature_scaler = MinMaxScaler()\n",
    "for i in features:\n",
    "    data[i] = feature_scaler.fit_transform(pd.DataFrame(data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale price data to 0-1 range\n",
    "label_scaler = MinMaxScaler()\n",
    "data['RRP5MIN'] = label_scaler.fit_transform(data['RRP5MIN'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data['2016-12-25 00:00:00':].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include time lags of timeseries data for last day i.e. 288 data points at 5 minutes granularity\n",
    "# Also 80 lags of same day previous week\n",
    "\n",
    "# Creating Daily lags\n",
    "for i in range(1,201):\n",
    "    train[\"price_l_{}\".format(i)] = train[\"DIFF_PRICE\"].shift(i)\n",
    "    train[\"demand_l_{}\".format(i)] = train[\"RESIDUAL_DEMAND\"].shift(i)\n",
    "    train[\"avgPrice_l_{}\".format(i)] = train[\"AVG_PRICE\"].shift(i)\n",
    "    \n",
    "\n",
    "# Creating Daily lags\n",
    "for i in range(255,325):\n",
    "    train[\"price_l_{}\".format(i)] = train[\"DIFF_PRICE\"].shift(i)\n",
    "    train[\"demand_l_{}\".format(i)] = train[\"RESIDUAL_DEMAND\"].shift(i)\n",
    "    train[\"avgPrice_l_{}\".format(i)] = train[\"AVG_PRICE\"].shift(i)\n",
    "        \n",
    "    \n",
    "# Creating Week ago lags\n",
    "j = 1\n",
    "size = 2016\n",
    "for i in range(size, size-65, -1):\n",
    "    train[\"w_price_l_{}\".format(j)] = train[\"DIFF_PRICE\"].shift(i)\n",
    "    train[\"w_demand_l_{}\".format(j)] = train[\"RESIDUAL_DEMAND\"].shift(i)\n",
    "    train[\"w_avgPrice_l_{}\".format(j)] = train[\"AVG_PRICE\"].shift(i)\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NANS\n",
    "train.dropna(inplace=True)\n",
    "train.head(5)\n",
    "train = train[\"2017\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### PROCESSING THE DATA FOR MLP NETWORK ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### THIS IS FOR MULTILAYER PERCEPTRON PURPOSES\n",
    "train1 = data[['hour', 'weekday', 'month', 'business hour', 'public holiday']]\n",
    "train1 = train1[\"2017\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the categorical variables using the same scaler used for LSTM variables\n",
    "cont = ['hour', 'weekday', 'month', 'business hour', 'public holiday']\n",
    "for i in cont:\n",
    "    train1[i] = feature_scaler.transform(pd.DataFrame(train1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = train1[train1.index.minute == 0]\n",
    "features1 = features1[features1.index.hour == 0]\n",
    "\n",
    "# Seperating training and test data for Multi-Layer Perceptron Network\n",
    "features_train1 = features1[:'2018']\n",
    "features_test1 = features1['2019':'2019-06-30']\n",
    "\n",
    "# Reshaping the features and test data to NP-Array as per Keras input requirement\n",
    "features_train1 = features_train1.to_numpy().reshape(features_train1.shape[0], features_train1.shape[1])\n",
    "features_test1 = features_test1.to_numpy().reshape(features_test1.shape[0], features_test1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### PROCESSING THE DATA FOR LSTM NETWORK ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature and label dataframes\n",
    "prelim_features = train.drop(['RRP5MIN', 'RESIDUAL_DEMAND', 'AVG_PRICE', 'DIFF_PRICE', 'hour', 'weekday', 'month', 'business hour', 'public holiday'], axis=1)\n",
    "prelim_labels = pd.DataFrame(train[['RRP5MIN']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format labels to 24 hour output range\n",
    "for i in range(0, 288):\n",
    "    prelim_labels['t_{}'.format(i)] = prelim_labels['RRP5MIN'].shift(-i)\n",
    "prelim_labels.drop(['RRP5MIN'], axis=1, inplace=True)\n",
    "\n",
    "# apply one-day discretization to the data\n",
    "labels = prelim_labels[prelim_labels.index.minute == 0]\n",
    "labels = labels[labels.index.hour == 0]\n",
    "features = prelim_features[prelim_features.index.minute == 0]\n",
    "features = features[features.index.hour == 0]\n",
    "\n",
    "features_train = features[:'2018']\n",
    "features_test = features['2019':'2019-06-30']\n",
    "labels_train = labels[:'2018']\n",
    "\n",
    "samples_train = len(features_train)\n",
    "samples_test = len(features_test)\n",
    "timesteps = 335\n",
    "\n",
    "# convert pandas data frames to numpy ndarrays\n",
    "features_train = features_train.to_numpy().reshape(samples_train, timesteps, 3)\n",
    "features_test = features_test.to_numpy().reshape(samples_test, timesteps, 3)\n",
    "labels_train = labels_train.to_numpy()\n",
    "\n",
    "# check for correct data shape\n",
    "features_train.shape, labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "########### CONCATENATE THE 2 NN & COMPILE THEM TO FORM BIGGER NN ##########\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the 2 models\n",
    "mlp = cm.create_mlp((features_train1.shape[1],))\n",
    "lstm = cm.create_conv_lstm((None, features_train.shape[1], 3))                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the 2 networks into a bigger network \n",
    "combinedInput = concatenate([mlp.output, lstm.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the bigger Network to the output layer to predict one-day ahead i.e. 288 intervals\n",
    "x = Dense(32, activation=\"sigmoid\")(combinedInput)\n",
    "x = Dense(288, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=[mlp.input, lstm.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model with Mean Absolute Error as loss function and Adam as optimizer\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, hist = cm.train_predict_evaluate(model, features_train1, features_train, labels_train, \n",
    "                                       features_test1, features_test, X_test, X_test.index, label_scaler,\n",
    "                                       32, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of predictions against Actuals\n",
    "cm.plot_chart(results[\"2019-01-01\":\"2019-01-25\"], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss comparision plot\n",
    "cm.plot_chart(pd.DataFrame(hist.history), xlab='Training Epoch', ylab='Mean Squared Error', title='Training and Validation Error over the Course of Training', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantifying Performance using MAE, MSE, RMSE\n",
    "cm.quantify_performance(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST LATER\n",
    "\n",
    "# def double_network(dense1, dense2, conv1, lstm1, dropout, act_func, dim_1, dim_2):\n",
    "#     # Creates the MLP model to perform regression on the categorical variables\n",
    "\n",
    "#     # define our MLP network\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(dense1, input_shape=dim_1, activation=\"relu\"))\n",
    "#     model.add(Dense(dense2, activation=\"relu\"))\n",
    "#     # check to see if the regression node should be added\n",
    "#     model_1 = model\n",
    "\n",
    "#     # Creates the Conv-LSTM model to perform Time-Series analysis\n",
    "#     conv_input_layer = Input(batch_shape=dim_2)\n",
    "\n",
    "#     x = Conv1D(conv1, kernel_size=335, strides=335, padding='valid')(conv_input_layer)\n",
    "#     x = Dropout(dropout)(x)\n",
    "#     x = LSTM(lstm1, recurrent_activation=act_func)(x)\n",
    "#     x = Dense(dense2 , activation=act_func)(x)\n",
    "#     model = Model(inputs=[conv_input_layer], outputs=[x])\n",
    "#     model_2 = model\n",
    " \n",
    "#     combinedInput = concatenate([model_1.output, model_2.output])\n",
    "    \n",
    "#     x = Dense(dense2, activation=act_func)(combinedInput)\n",
    "#     x = Dense(288, activation=act_func)(x)\n",
    "#     model = Model(inputs=[mlp.input, lstm.input], outputs=x)\n",
    "#     model.compile(loss='mae', optimizer='adam')\n",
    "#     return rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TEST LATER\n",
    "\n",
    "# ######## CHECKING MODELS ##########\n",
    "# models_list = []\n",
    "# # small model\n",
    "# models_list.append(double_network(64, 32, 64, 64, 0.1, 'sigmoid', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "# models_list.append(double_network(64, 32, 64, 64, 0.2, 'sigmoid', dim_1=(features_train1.shape[1],), \n",
    "# #                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "# models_list.append(double_network(64, 32, 64, 64, 0.1, 'relu', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "# models_list.append(double_network(64, 32, 64, 64, 0.2, 'relu', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "\n",
    "# # Smaller\n",
    "# models_list.append(double_network(32, 32, 32, 32, 0.1, 'sigmoid', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "# models_list.append(double_network(32, 32, 32, 32, 0.2, 'sigmoid', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "# models_list.append(double_network(64, 32, 32, 32, 0.1, 'relu', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "# models_list.append(double_network(64, 32, 32, 32, 0.2, 'relu', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "\n",
    "# # Going bigger\n",
    "# models_list.append(double_network(64, 64, 64, 64, 0.1, 'sigmoid', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "# models_list.append(double_network(64, 64, 64, 64, 0.2, 'sigmoid', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "# models_list.append(double_network(64, 64, 64, 64, 0.1, 'relu', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "# models_list.append(cm.double_network(64, 64, 64, 64, 0.2, 'relu', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "\n",
    "# # Even Bigger\n",
    "# models_list.append(double_network(64, 64, 128, 64, 0.1, 'sigmoid', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "# models_list.append(double_network(64, 64, 128, 64, 0.2, 'sigmoid', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "# models_list.append(double_network(64, 64, 64, 64, 0.1, 'relu', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "# models_list.append(double_network(64, 64, 64, 64, 0.2, 'relu', dim_1=(features_train1.shape[1],), \n",
    "#                                      dim_2=(None, features_train.shape[1], 3)))\n",
    "\n",
    "\n",
    "\n",
    "# # train all archtitectures and evaluate performance on the test set\n",
    "# for i, rnn in enumerate(models_list):\n",
    "\n",
    "#     results, hist = cm.train_predict_evaluate(model, features_train1, features_train, labels_train, \n",
    "#                                        features_test1, features_test, X_test, X_test.index, label_scaler,\n",
    "#                                        32, 160)\n",
    "#     print(\"Model {}\".format(i))\n",
    "#     cm.quantify_performance(results)\n",
    "#     print(\"--------------------------------------------\")\n",
    "#     print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input prices into optimizer\n",
    "import optimizer_module as om\n",
    "numDays = 100 # Number of days to run model\n",
    "start = 0 # Starting time interval from price data\n",
    "bStorage0 = 0 # Starting battery charge\n",
    "\n",
    "predPrices = results.iloc[start:start+(numDays*288)][\"prediction\"].tolist()\n",
    "realPrices = results.iloc[start:start+(numDays*288)][\"true values\"].tolist()\n",
    "outputResults = 1\n",
    "outputActions = 1\n",
    "\n",
    "print(\"Optimizer results for real prices\")\n",
    "realNxtAction, realNxtBatCharge, realActions = om.optimize(realPrices, bStorage0, outputResults, outputActions)\n",
    "\n",
    "print(\"\\nOptimizer results for predicted prices\\nProfit is incorrect as it is calculating predicted profit not actual profit\")\n",
    "predNxtAction, predNxtBatCharge, predActions = om.optimize(predPrices, bStorage0, outputResults, outputActions)\n",
    "\n",
    "maxProfit = sum([realActions[i]*realPrices[i]/12 for i in range(numDays*288)])\n",
    "actualProfit = sum([predActions[i]*realPrices[i]/12 for i in range(numDays*288)])\n",
    "\n",
    "print(\"\\n----------RESULTS----------\")\n",
    "print(\"Max profit possible: $%.4g\" % (maxProfit))\n",
    "print(\"Actual profit: $%.4g -> %.4g%% of max profit possible\" % (actualProfit,actualProfit/maxProfit*100))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
