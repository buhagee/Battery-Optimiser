{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import seaborn as sns\n",
    "import custom_module as cm\n",
    "#import optimizer_module as om"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "####################### DATA PREPROCESSING #################################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot do slice indexing on RangeIndex with these indexers [2016] of type str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6ebe2e5e6fff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Loading the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"combined_data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"2016\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2880\u001b[0m         \u001b[1;31m# Do we have a slicer (on rows)?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2881\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_index_sliceable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2882\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2883\u001b[0m             \u001b[1;31m# either we have a slice or we have a string that can be converted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36mconvert_to_index_sliceable\u001b[1;34m(obj, key)\u001b[0m\n\u001b[0;32m   2132\u001b[0m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2134\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_slice_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2136\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_convert_slice_indexer\u001b[1;34m(self, key, kind)\u001b[0m\n\u001b[0;32m   3154\u001b[0m             \"\"\"\n\u001b[0;32m   3155\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_index_slice\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3156\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"slice\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3157\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"slice\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3158\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"slice\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_validate_indexer\u001b[1;34m(self, form, key, kind)\u001b[0m\n\u001b[0;32m   4995\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4996\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4997\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalid_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4999\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_maybe_cast_slice_bound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_invalid_indexer\u001b[1;34m(self, form, key)\u001b[0m\n\u001b[0;32m   3266\u001b[0m         \"\"\"\n\u001b[0;32m   3267\u001b[0m         raise TypeError(\n\u001b[1;32m-> 3268\u001b[1;33m             \u001b[1;34mf\"cannot do {form} indexing on {type(self).__name__} with these \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3269\u001b[0m             \u001b[1;34mf\"indexers [{key}] of type {type(key).__name__}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3270\u001b[0m         )\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot do slice indexing on RangeIndex with these indexers [2016] of type str"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "data = pd.read_csv(\"combined_data.csv\", parse_dates=True)\n",
    "data = data[\"2016\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load this file for saving time\n",
    "# Saving the data file so we can reload with the features made again to reduce time\n",
    "# data.to_csv('final.csv') \n",
    "data = pd.read_csv(\"final.csv\", parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the data to index\n",
    "data[\"SETTLEMENTDATE\"] = pd.to_datetime(data[\"SETTLEMENTDATE\"])\n",
    "data.index = data[\"SETTLEMENTDATE\"]\n",
    "data.drop(columns=\"SETTLEMENTDATE\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace outliers by outlier threshold\n",
    "data = cm.replace_outliers(data, 'RRP5MIN', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the test dataset for testing purposes in evaluation\n",
    "X_test = data[\"2019-01-01\":\"2019-06-30\"].copy()\n",
    "X_test = X_test[\"RRP5MIN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### SOME CHARTS OF THE DATA IN HAND ####################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curve Price\n",
    "cm.plot_chart(data[\"RRP5MIN\"].loc[\"2018-02-20\":\"2018-02-21\"], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curve Residual Demand\n",
    "# Looks stationary\n",
    "cm.plot_chart(data[\"RESIDUAL_DEMAND\"].loc[\"2019\"], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MORE IF YOU WANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MORE IF YOU WANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "####################### FEATURES PREPROCESSING #############################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg Price of last 1 hour i.e. 12 data points at 5 minutes granularity\n",
    "data[\"AVG_PRICE\"] = pd.DataFrame(cm.average_hours(data[\"RRP5MIN\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differencing the average price and creating a differenced price variable\n",
    "data[\"AVG_PRICE\"] = cm.period_difference(data[\"AVG_PRICE\"])\n",
    "data[\"DIFF_PRICE\"] = cm.period_difference(data[\"RRP5MIN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'hour', 'weekday' and 'month' features\n",
    "data['hour'] = 0\n",
    "data['weekday'] = 0\n",
    "data['month'] = 0\n",
    "for i in range(len(data)):\n",
    "    position = data.index[i]\n",
    "    data['hour'][i] = position.hour\n",
    "    data['weekday'][i] = position.weekday()\n",
    "    data['month'][i] = position.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING FEATURES\n",
    "# Generate 'business hour' feature. 7am-7pm business hours\n",
    "data[\"business hour\"] = 0\n",
    "for i in range(len(data)):\n",
    "    position = data.index[i]\n",
    "    hour = position.hour\n",
    "    if ((hour > 7 and hour < 12) or (hour > 14 and hour < 19)):\n",
    "        data[\"business hour\"][i] = 2\n",
    "    elif (hour >= 12 and hour <= 14):\n",
    "        data[\"business hour\"][i] = 1\n",
    "    else:\n",
    "        data[\"business hour\"][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'weekend' feature\n",
    "for i in range(len(data)):\n",
    "    position = data.index[i]\n",
    "    weekday = position.weekday()\n",
    "    if (weekday == 6):\n",
    "        data['weekday'][i] = 2\n",
    "    elif (weekday == 5):\n",
    "        data['weekday'][i] = 1\n",
    "    else:\n",
    "        data['weekday'][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "#pip install holidays\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus_holidays = holidays.CountryHoliday('AUS', prov='NSW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"public holiday\"] = 0\n",
    "for i in range(len(data)):\n",
    "    if (data.index[i] in aus_holidays):\n",
    "        data[\"public holiday\"][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SAVE FILE HERE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "########### PREPARING DATA FOR KERAS TO PROCESS PREPROCESSING ##############\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## 2 MODELS #######################################################\n",
    "######### 1st for processing Categorical Data for Regression via Multi-Layer Perceptron #########\n",
    "########################### 2nd for processing Time Series via LSTM ##############################\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the RRP between 0 and 1 as required by the NN\n",
    "features = ['RESIDUAL_DEMAND', 'AVG_PRICE', 'DIFF_PRICE']\n",
    "feature_scaler = MinMaxScaler()\n",
    "for i in features:\n",
    "    data[i] = feature_scaler.fit_transform(pd.DataFrame(data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale price data to 0-1 range\n",
    "label_scaler = MinMaxScaler()\n",
    "data['RRP5MIN'] = label_scaler.fit_transform(data['RRP5MIN'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data['2016-12-25 00:00:00':].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include time lags of timeseries data for last day i.e. 288 data points at 5 minutes granularity\n",
    "# Also 80 lags of same day previous week\n",
    "\n",
    "# Creating Daily lags\n",
    "for i in range(1,201):\n",
    "    train[\"price_l_{}\".format(i)] = train[\"DIFF_PRICE\"].shift(i)\n",
    "    train[\"demand_l_{}\".format(i)] = train[\"RESIDUAL_DEMAND\"].shift(i)\n",
    "    train[\"avgPrice_l_{}\".format(i)] = train[\"AVG_PRICE\"].shift(i)\n",
    "    \n",
    "\n",
    "# Creating Week ago lags\n",
    "j = 1\n",
    "size = 2016\n",
    "for i in range(size, size-80, -1):\n",
    "    train[\"w_price_l_{}\".format(j)] = train[\"DIFF_PRICE\"].shift(i)\n",
    "    train[\"w_demand_l_{}\".format(j)] = train[\"RESIDUAL_DEMAND\"].shift(i)\n",
    "    train[\"w_avgPrice_l_{}\".format(j)] = train[\"AVG_PRICE\"].shift(i)\n",
    "    j+=1\n",
    "\n",
    "\n",
    "# # Creating Month ago lags\n",
    "# j = 1\n",
    "# for i in range(1728,2016):\n",
    "#     train[\"w_l_{}\".format(j)] = train[\"RRP5MIN\"].shift(i)\n",
    "#     j+=1\n",
    "        \n",
    "#Adjustment for leap year required Here!!!!!!!!!!\n",
    "#Creating year ago lags\n",
    "# j = 1\n",
    "# size = data['2017'].shape[0]\n",
    "# for i in range(size, size-50, -1):\n",
    "#     train[\"y_price_l_{}\".format(j)] = train[\"DIFF_PRICE\"].shift(i)\n",
    "#     train[\"y_demand_l_{}\".format(j)] = train[\"RESIDUAL_DEMAND\"].shift(i)\n",
    "#     train[\"y_avgPrice_l_{}\".format(j)] = train[\"AVG_PRICE\"].shift(i)\n",
    "#     j+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NANS\n",
    "train.dropna(inplace=True)\n",
    "train.head(5)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### PROCESSING THE DATA FOR MLP NETWORK ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### THIS IS FOR MULTILAYER PERCEPTRON PURPOSES\n",
    "train1 = data[['hour', 'weekday', 'month', 'business hour', 'public holiday', 'RRP5MIN']]\n",
    "train1 = train1[\"2017\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the categorical variables using the same scaler used for LSTM variables\n",
    "cont = ['hour', 'weekday', 'month', 'business hour', 'public holiday']\n",
    "for i in cont:\n",
    "    train1[i] = feature_scaler.transform(pd.DataFrame(train1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = train1[train1.index.minute == 0]\n",
    "features1 = features1[features1.index.hour == 0]\n",
    "\n",
    "# Seperating training and test data for Multi-Layer Perceptron Network\n",
    "features_train1 = features1[:'2018']\n",
    "features_test1 = features1['2019':'2019-06-30']\n",
    "\n",
    "# Reshaping the features and test data to NP-Array as per Keras input requirement\n",
    "features_train1 = features_train1.to_numpy().reshape(features_train1.shape[0], features_train1.shape[1])\n",
    "features_test1 = features_test1.to_numpy().reshape(features_test1.shape[0], features_test1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### PROCESSING THE DATA FOR LSTM NETWORK ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature and label dataframes\n",
    "prelim_features = train.drop(['RRP5MIN', 'RESIDUAL_DEMAND', 'AVG_PRICE', 'DIFF_PRICE', 'hour', 'weekday', 'month', 'business hour', 'public holiday'], axis=1)\n",
    "prelim_labels = pd.DataFrame(train[['RRP5MIN']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format labels to 24 hour output range\n",
    "for i in range(0, 288):\n",
    "    prelim_labels['t_{}'.format(i)] = prelim_labels['RRP5MIN'].shift(-i)\n",
    "prelim_labels.drop(['RRP5MIN'], axis=1, inplace=True)\n",
    "\n",
    "# apply one-day discretization to the data\n",
    "labels = prelim_labels[prelim_labels.index.minute == 0]\n",
    "labels = labels[labels.index.hour == 0]\n",
    "features = prelim_features[prelim_features.index.minute == 0]\n",
    "features = features[features.index.hour == 0]\n",
    "\n",
    "features_train = features[:'2018']\n",
    "features_test = features['2019':'2019-06-30']\n",
    "labels_train = labels[:'2018']\n",
    "\n",
    "samples_train = len(features_train)\n",
    "samples_test = len(features_test)\n",
    "timesteps = 280\n",
    "\n",
    "# convert pandas data frames to numpy ndarrays\n",
    "features_train = features_train.to_numpy().reshape(samples_train, timesteps, 3)\n",
    "features_test = features_test.to_numpy().reshape(samples_test, timesteps, 3)\n",
    "labels_train = labels_train.to_numpy()\n",
    "\n",
    "# check for correct data shape\n",
    "features_train.shape, labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import json\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and validation data\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(features_train, labels_train, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model.compile(loss='mae', optimizer='adam')\n",
    "# checkpoint = ModelCheckpoint('./models/multidim_timeseries_testing.hdf5', save_best_only=True)\n",
    "\n",
    "# hist = rnn.fit(X_train, y_train,\n",
    "#                  validation_data=(X_valid, y_valid),\n",
    "#                  callbacks=[checkpoint], \n",
    "#                  verbose=1, batch_size=16, epochs=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn = cm.RNN_model(64, 64, 128, input_s=(X_train.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='mae', optimizer='adam')\n",
    "# # checkpoint = ModelCheckpoint('./models/multidim_timeseries_testing.hdf5', save_best_only=True)\n",
    "\n",
    "# hist = model.fit(X_train, y_train,\n",
    "#                  validation_data=(X_valid, y_valid),\n",
    "#                  callbacks=[checkpoint], \n",
    "#                  verbose=1, batch_size=50, epochs=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # results, hist = cm.train_predict_evaluate(rnn, X_train, X_valid, y_train, y_valid, features_test,\n",
    "#                                        X_test.to_numpy().flatten(), X_test.index, label_scaler, 32, 160, \n",
    "#                                        'multidim_timeseries_testing.hdf5', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best = load_model('./models/multidim_timeseries_testing.hdf5')\n",
    "# # pred = best.predict([input_test, input_test[:, :, 3]])\n",
    "# pred = best.predict(features_test)\n",
    "# #pred = scaler.inverse_transform(pred.flatten().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "########### CONCATENATE THE 2 NN & COMPILE THEM TO FORM BIGGER NN ##########\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the 2 models\n",
    "mlp = cm.create_mlp((features_train1.shape[1],))\n",
    "lstm = cm.create_conv_lstm((None, features_train.shape[1], 3))                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the 2 networks into a bigger network \n",
    "combinedInput = concatenate([mlp.output, lstm.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the bigger Network to the output layer to predict one-day ahead i.e. 288 intervals\n",
    "x = Dense(32, activation=\"relu\")(combinedInput)\n",
    "x = Dense(288, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=[mlp.input, lstm.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model with Mean Absolute Error as loss function and Adam as optimizer\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "#checkpoint = ModelCheckpoint('./models/multidim_timeseries_testing.hdf5', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "hist = model.fit(x=[features_train1, features_train], y=labels_train, \n",
    "                 verbose=1, batch_size=50, epochs=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions on the Testing Data\n",
    "pred = model.predict([features_test1, features_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse scaling the predictions and re-shaping it to 1D output vector\n",
    "pred = label_scaler.inverse_transform(pred.flatten().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Predictions and True Values in results dataframe\n",
    "results = pd.DataFrame({'prediction':pred.flatten(), 'true values':X_test}, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of predictions against Actuals\n",
    "cm.plot_chart(results[\"2019-01-01\":\"2019-01-10\"], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss comparision plot\n",
    "cm.plot_chart(pd.DataFrame(hist.history), xlab='Training Epoch', ylab='Mean Squared Error', title='Training and Validation Error over the Course of Training', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantifying Performance using MAE, MSE, RMSE\n",
    "cm.quantify_performance(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CHECKING MODELS ##########\n",
    "models_list = []\n",
    "models_list.append(cm.RNN_model(64, 64, 128, input_s=(X_train.shape[1],1)))\n",
    "models_list.append(cm.RNN_model(128, 32, 64, input_s=(X_train.shape[1],1)))\n",
    "models_list.append(cm.RNN_model(64, 32, 64, input_s=(X_train.shape[1],1)))\n",
    "models_list.append(cm.RNN_model(32, 16, 32, input_s=(X_train.shape[1],1)))\n",
    "models_list.append(cm.RNN_model(128, 32, 64, input_s=(X_train.shape[1],1)))\n",
    "models_list.append(cm.RNN_model(128, 64, 128, input_s=(X_train.shape[1],1)))\n",
    "# train all archtitectures and evaluate performance on the test set\n",
    "for i, rnn in enumerate(models_list):\n",
    "\n",
    "    results, hist = cm.train_predict_evaluate(rnn, X_train, X_valid, y_train, y_valid, features_test,\n",
    "                                       X_test.to_numpy().flatten(), X_test.index, label_scaler, 32, 160, \n",
    "                                       'timeseries_architecture_tests.hdf5', verbose=0)\n",
    "    print(\"Model {}\".format(i))\n",
    "    cm.quantify_performance(results)\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = train_test_split(features_train, features_train1, test_size=0.25, random_state=42)\n",
    "# (features_training, features_testing, features_training1, features_testing1) = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### SEPERATING Y TRAIN AND Y TEST FOR VALIDATION PURPOSES\n",
    "# trainY = features_training1[\"RRP5MIN\"]\n",
    "# testY = features_testing1[\"RRP5MIN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_training1.drop(columns=[\"RRP5MIN\"], inplace=True)\n",
    "# features_testing1.drop(columns=[\"RRP5MIN\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_training1 = features_training1.to_numpy().reshape(features_training1.shape[0], features_training1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_testing1 = features_testing1.to_numpy().reshape(features_testing1.shape[0], features_testing1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Hour of the day\n",
    "# Day of the week\n",
    "# Flag of Business Hour\n",
    "# Public Holiday Flag\n",
    "# Forecast Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"2019\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input prices into optimizer\n",
    "import optimizer_module as om\n",
    "numDays = 100 # Number of days to run model\n",
    "start = 0 # Starting time interval from price data\n",
    "bStorage0 = 0 # Starting battery charge\n",
    "\n",
    "predPrices = results.iloc[start:start+(numDays*288)][\"prediction\"].tolist()\n",
    "realPrices = results.iloc[start:start+(numDays*288)][\"true values\"].tolist()\n",
    "outputResults = 1\n",
    "outputActions = 1\n",
    "\n",
    "print(\"Optimizer results for real prices\")\n",
    "realNxtAction, realNxtBatCharge, realActions = om.optimize(realPrices, bStorage0, outputResults, outputActions)\n",
    "\n",
    "print(\"\\nOptimizer results for predicted prices\\nProfit is incorrect as it is calculating predicted profit not actual profit\")\n",
    "predNxtAction, predNxtBatCharge, predActions = om.optimize(predPrices, bStorage0, outputResults, outputActions)\n",
    "\n",
    "maxProfit = sum([realActions[i]*realPrices[i]/12 for i in range(numDays*288)])\n",
    "actualProfit = sum([predActions[i]*realPrices[i]/12 for i in range(numDays*288)])\n",
    "\n",
    "print(\"\\n----------RESULTS----------\")\n",
    "print(\"Max profit possible: $%.4g\" % (maxProfit))\n",
    "print(\"Actual profit: $%.4g -> %.4g%% of max profit possible\" % (actualProfit,actualProfit/maxProfit*100))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
