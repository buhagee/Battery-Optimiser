{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import seaborn as sns\n",
    "import custom_module as cm\n",
    "import optimizer_module as om"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "####################### DATA PREPROCESSING #################################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "data = pd.read_csv(\"combined_data.csv\", parse_dates=True)\n",
    "#data = data[\"2016\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load this file for saving time\n",
    "# Saving the data file so we can reload with the features made again to reduce time\n",
    "# data.to_csv('final.csv') \n",
    "data = pd.read_csv(\"final.csv\", parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RRP5MIN</th>\n",
       "      <th>RESIDUAL_DEMAND</th>\n",
       "      <th>AVG_PRICE</th>\n",
       "      <th>DIFF_PRICE</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>business hour</th>\n",
       "      <th>public holiday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SETTLEMENTDATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>4.096091</td>\n",
       "      <td>1593.93</td>\n",
       "      <td>5.046193</td>\n",
       "      <td>-0.486409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:05:00</th>\n",
       "      <td>5.257498</td>\n",
       "      <td>1557.06</td>\n",
       "      <td>5.102443</td>\n",
       "      <td>1.161408</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:10:00</th>\n",
       "      <td>5.415838</td>\n",
       "      <td>1510.10</td>\n",
       "      <td>5.087579</td>\n",
       "      <td>0.158340</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:15:00</th>\n",
       "      <td>5.415838</td>\n",
       "      <td>1474.70</td>\n",
       "      <td>5.087579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:20:00</th>\n",
       "      <td>3.609315</td>\n",
       "      <td>1464.90</td>\n",
       "      <td>4.937838</td>\n",
       "      <td>-1.806523</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30 23:40:00</th>\n",
       "      <td>5.998145</td>\n",
       "      <td>546.95</td>\n",
       "      <td>4.556324</td>\n",
       "      <td>-0.136612</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30 23:45:00</th>\n",
       "      <td>5.961220</td>\n",
       "      <td>560.19</td>\n",
       "      <td>4.778101</td>\n",
       "      <td>-0.036925</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30 23:50:00</th>\n",
       "      <td>5.066510</td>\n",
       "      <td>559.87</td>\n",
       "      <td>4.907920</td>\n",
       "      <td>-0.894710</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30 23:55:00</th>\n",
       "      <td>5.002401</td>\n",
       "      <td>547.07</td>\n",
       "      <td>5.023500</td>\n",
       "      <td>-0.064109</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01 00:00:00</th>\n",
       "      <td>5.063096</td>\n",
       "      <td>570.31</td>\n",
       "      <td>5.158584</td>\n",
       "      <td>0.060695</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367777 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      RRP5MIN  RESIDUAL_DEMAND  AVG_PRICE  DIFF_PRICE  hour  \\\n",
       "SETTLEMENTDATE                                                                \n",
       "2016-01-01 00:00:00  4.096091          1593.93   5.046193   -0.486409     0   \n",
       "2016-01-01 00:05:00  5.257498          1557.06   5.102443    1.161408     0   \n",
       "2016-01-01 00:10:00  5.415838          1510.10   5.087579    0.158340     0   \n",
       "2016-01-01 00:15:00  5.415838          1474.70   5.087579    0.000000     0   \n",
       "2016-01-01 00:20:00  3.609315          1464.90   4.937838   -1.806523     0   \n",
       "...                       ...              ...        ...         ...   ...   \n",
       "2019-06-30 23:40:00  5.998145           546.95   4.556324   -0.136612    23   \n",
       "2019-06-30 23:45:00  5.961220           560.19   4.778101   -0.036925    23   \n",
       "2019-06-30 23:50:00  5.066510           559.87   4.907920   -0.894710    23   \n",
       "2019-06-30 23:55:00  5.002401           547.07   5.023500   -0.064109    23   \n",
       "2019-07-01 00:00:00  5.063096           570.31   5.158584    0.060695     0   \n",
       "\n",
       "                     weekday  month  business hour  public holiday  \n",
       "SETTLEMENTDATE                                                      \n",
       "2016-01-01 00:00:00        0      1              0               1  \n",
       "2016-01-01 00:05:00        0      1              0               1  \n",
       "2016-01-01 00:10:00        0      1              0               1  \n",
       "2016-01-01 00:15:00        0      1              0               1  \n",
       "2016-01-01 00:20:00        0      1              0               1  \n",
       "...                      ...    ...            ...             ...  \n",
       "2019-06-30 23:40:00        0      6              0               0  \n",
       "2019-06-30 23:45:00        0      6              0               0  \n",
       "2019-06-30 23:50:00        0      6              0               0  \n",
       "2019-06-30 23:55:00        0      6              0               0  \n",
       "2019-07-01 00:00:00        0      7              0               0  \n",
       "\n",
       "[367777 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SETTLEMENTDATE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SETTLEMENTDATE'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ea3b3e8a8bf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Setting the data to index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SETTLEMENTDATE\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SETTLEMENTDATE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SETTLEMENTDATE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"SETTLEMENTDATE\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SETTLEMENTDATE'"
     ]
    }
   ],
   "source": [
    "# Setting the data to index\n",
    "data[\"SETTLEMENTDATE\"] = pd.to_datetime(data[\"SETTLEMENTDATE\"])\n",
    "data.index = data[\"SETTLEMENTDATE\"]\n",
    "data.drop(columns=\"SETTLEMENTDATE\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace outliers by outlier threshold\n",
    "data = cm.replace_outliers(data, 'RRP5MIN', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the test dataset for testing purposes in evaluation\n",
    "X_test = data[\"2019-01-01\":\"2019-06-30\"].copy()\n",
    "X_test = X_test[\"RRP5MIN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### SOME CHARTS OF THE DATA IN HAND ####################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curve Price\n",
    "cm.plot_chart(data[\"RRP5MIN\"].loc[\"2018-02-20\":\"2018-02-21\"], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curve Residual Demand\n",
    "# Looks stationary\n",
    "cm.plot_chart(data[\"RESIDUAL_DEMAND\"].loc[\"2019\"], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether the series is Stationary via Dickey Fuller Test\n",
    "# TESTING FOR PRICE\n",
    "cm.is_Stationary(data.loc[\"2018\"][\"RRP5MIN\"])\n",
    "# TESTING FOR RESIDUAL DEMAND\n",
    "cm.is_Stationary(data.loc[\"2018\"][\"RESIDUAL_DEMAND\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function plots the ACF and PACF plots to find useful lags for time series\n",
    "cm.serial_corr(data.loc[\"2018\"]['RRP5MIN'], lags=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ADD CORRELATION PLOT HERE WITH OTHER VARIABLES #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MORE IF YOU WANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "####################### FEATURES PREPROCESSING #############################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg Price of last 1 hour i.e. 12 data points at 5 minutes granularity\n",
    "data[\"AVG_PRICE\"] = pd.DataFrame(cm.average_hours(data[\"RRP5MIN\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differencing the average price and creating a differenced price variable\n",
    "data[\"AVG_PRICE\"] = cm.period_difference(data[\"AVG_PRICE\"])\n",
    "data[\"DIFF_PRICE\"] = cm.period_difference(data[\"RRP5MIN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'hour', 'weekday' and 'month' features\n",
    "data['hour'] = 0\n",
    "data['weekday'] = 0\n",
    "data['month'] = 0\n",
    "for i in range(len(data)):\n",
    "    position = data.index[i]\n",
    "    data['hour'][i] = position.hour\n",
    "    data['weekday'][i] = position.weekday()\n",
    "    data['month'][i] = position.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING FEATURES\n",
    "# Generate 'business hour' feature. 7am-7pm business hours\n",
    "data[\"business hour\"] = 0\n",
    "for i in range(len(data)):\n",
    "    position = data.index[i]\n",
    "    hour = position.hour\n",
    "    if ((hour > 7 and hour < 12) or (hour > 14 and hour < 19)):\n",
    "        data[\"business hour\"][i] = 2\n",
    "    elif (hour >= 12 and hour <= 14):\n",
    "        data[\"business hour\"][i] = 1\n",
    "    else:\n",
    "        data[\"business hour\"][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'weekend' feature\n",
    "for i in range(len(data)):\n",
    "    position = data.index[i]\n",
    "    weekday = position.weekday()\n",
    "    if (weekday == 6):\n",
    "        data['weekday'][i] = 2\n",
    "    elif (weekday == 5):\n",
    "        data['weekday'][i] = 1\n",
    "    else:\n",
    "        data['weekday'][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus_holidays = holidays.CountryHoliday('AUS', prov='SA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"public holiday\"] = 0\n",
    "for i in range(len(data)):\n",
    "    if (data.index[i] in aus_holidays):\n",
    "        data[\"public holiday\"][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SAVE FILE HERE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "########### PREPARING DATA FOR KERAS TO PROCESS PREPROCESSING ##############\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## 2 MODELS #######################################################\n",
    "######### 1st for processing Categorical Data for Regression via Multi-Layer Perceptron #########\n",
    "########################### 2nd for processing Time Series via LSTM ##############################\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the RRP between 0 and 1 as required by the NN\n",
    "features = ['RESIDUAL_DEMAND', 'AVG_PRICE', 'DIFF_PRICE']\n",
    "feature_scaler = MinMaxScaler()\n",
    "for i in features:\n",
    "    data[i] = feature_scaler.fit_transform(pd.DataFrame(data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale price data to 0-1 range\n",
    "label_scaler = MinMaxScaler()\n",
    "data['RRP5MIN'] = label_scaler.fit_transform(data['RRP5MIN'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data['2016-12-25 00:00:00':].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include time lags of timeseries data for last day i.e. 288 data points at 5 minutes granularity\n",
    "# Also 80 lags of same day previous week\n",
    "\n",
    "# Creating Daily lags\n",
    "for i in range(1,201):\n",
    "    train[\"price_l_{}\".format(i)] = train[\"DIFF_PRICE\"].shift(i)\n",
    "    train[\"demand_l_{}\".format(i)] = train[\"RESIDUAL_DEMAND\"].shift(i)\n",
    "    train[\"avgPrice_l_{}\".format(i)] = train[\"AVG_PRICE\"].shift(i)\n",
    "    \n",
    "\n",
    "# Creating Daily lags\n",
    "for i in range(255,325):\n",
    "    train[\"price_l_{}\".format(i)] = train[\"DIFF_PRICE\"].shift(i)\n",
    "    train[\"demand_l_{}\".format(i)] = train[\"RESIDUAL_DEMAND\"].shift(i)\n",
    "    train[\"avgPrice_l_{}\".format(i)] = train[\"AVG_PRICE\"].shift(i)\n",
    "        \n",
    "    \n",
    "# Creating Week ago lags\n",
    "j = 1\n",
    "size = 2016\n",
    "for i in range(size, size-65, -1):\n",
    "    train[\"w_price_l_{}\".format(j)] = train[\"DIFF_PRICE\"].shift(i)\n",
    "    train[\"w_demand_l_{}\".format(j)] = train[\"RESIDUAL_DEMAND\"].shift(i)\n",
    "    train[\"w_avgPrice_l_{}\".format(j)] = train[\"AVG_PRICE\"].shift(i)\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NANS\n",
    "train.dropna(inplace=True)\n",
    "train.head(5)\n",
    "train = train[\"2017\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### PROCESSING THE DATA FOR MLP NETWORK ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### THIS IS FOR MULTILAYER PERCEPTRON PURPOSES\n",
    "train1 = data[['hour', 'weekday', 'month', 'business hour', 'public holiday']]\n",
    "train1 = train1[\"2017\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the categorical variables using the same scaler used for LSTM variables\n",
    "cont = ['hour', 'weekday', 'month', 'business hour', 'public holiday']\n",
    "for i in cont:\n",
    "    train1[i] = feature_scaler.transform(pd.DataFrame(train1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = train1[train1.index.minute == 0]\n",
    "features1 = features1[features1.index.hour == 0]\n",
    "\n",
    "# Seperating training and test data for Multi-Layer Perceptron Network\n",
    "features_train1 = features1[:'2018']\n",
    "features_test1 = features1['2019':'2019-06-30']\n",
    "\n",
    "# Reshaping the features and test data to NP-Array as per Keras input requirement\n",
    "features_train1 = features_train1.to_numpy().reshape(features_train1.shape[0], features_train1.shape[1])\n",
    "features_test1 = features_test1.to_numpy().reshape(features_test1.shape[0], features_test1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### PROCESSING THE DATA FOR LSTM NETWORK ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature and label dataframes\n",
    "prelim_features = train.drop(['RRP5MIN', 'RESIDUAL_DEMAND', 'AVG_PRICE', 'DIFF_PRICE', 'hour', 'weekday', 'month', 'business hour', 'public holiday'], axis=1)\n",
    "prelim_labels = pd.DataFrame(train[['RRP5MIN']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format labels to 24 hour output range\n",
    "for i in range(0, 288):\n",
    "    prelim_labels['t_{}'.format(i)] = prelim_labels['RRP5MIN'].shift(-i)\n",
    "prelim_labels.drop(['RRP5MIN'], axis=1, inplace=True)\n",
    "\n",
    "# apply one-day discretization to the data\n",
    "labels = prelim_labels[prelim_labels.index.minute == 0]\n",
    "labels = labels[labels.index.hour == 0]\n",
    "features = prelim_features[prelim_features.index.minute == 0]\n",
    "features = features[features.index.hour == 0]\n",
    "\n",
    "features_train = features[:'2018']\n",
    "features_test = features['2019':'2019-06-30']\n",
    "labels_train = labels[:'2018']\n",
    "\n",
    "samples_train = len(features_train)\n",
    "samples_test = len(features_test)\n",
    "timesteps = 335\n",
    "\n",
    "# convert pandas data frames to numpy ndarrays\n",
    "features_train = features_train.to_numpy().reshape(samples_train, timesteps, 3)\n",
    "features_test = features_test.to_numpy().reshape(samples_test, timesteps, 3)\n",
    "labels_train = labels_train.to_numpy()\n",
    "\n",
    "# check for correct data shape\n",
    "features_train.shape, labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "########### CONCATENATE THE 2 NN & COMPILE THEM TO FORM BIGGER NN ##########\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the 2 models\n",
    "mlp_shape = (features_train1.shape[1],)\n",
    "lstm_shape = (None, features_train.shape[1], 3)\n",
    "\n",
    "# mlp = cm.create_mlp((features_train1.shape[1],))\n",
    "# lstm = cm.create_conv_lstm((None, features_train.shape[1], 3))      \n",
    "\n",
    "mlp = cm.create_mlp(mlp_shape)\n",
    "lstm = cm.create_conv_lstm(lstm_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the 2 networks into a bigger network \n",
    "combinedInput = concatenate([mlp.output, lstm.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the bigger Network to the output layer to predict one-day ahead i.e. 288 intervals\n",
    "x = Dense(32, activation=\"relu\")(combinedInput)\n",
    "x = Dense(288, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=[mlp.input, lstm.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model with Mean Absolute Error as loss function and Adam as optimizer\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, hist = cm.train_predict_evaluate(model, features_train1, features_train, labels_train, \n",
    "                                       features_test1, features_test, X_test, X_test.index, label_scaler,\n",
    "                                       32, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of predictions against Actuals\n",
    "cm.plot_chart(results[\"2019-01-01\":\"2019-01-25\"], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss comparision plot\n",
    "cm.plot_chart(pd.DataFrame(hist.history), xlab='Training Epoch', ylab='Mean Squared Error', title='Training and Validation Error over the Course of Training', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantifying Performance using MAE, MSE, RMSE\n",
    "cm.quantify_performance(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "################## FINE TUNE THE MODEL ###################\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our function to find the work with different parameters\n",
    "def fine_tune(mlp_dense1, conv1D_dense, lstm_dense, dense2, activation_func, \n",
    "             dropout_rate, loss, mlp_shape, lstm_shape):\n",
    "    # Creating the 2 models\n",
    "    mlp = cm.create_mlp(mlp_shape, mlp_dense1, dense2)\n",
    "    lstm = cm.create_conv_lstm(lstm_shape, conv1D_dense, lstm_dense, dense2, \n",
    "                               activation_func, dropout_rate)\n",
    "    \n",
    "    # Merging the 2 networks into a bigger network \n",
    "    combinedInput = concatenate([mlp.output, lstm.output])\n",
    "    \n",
    "        # Mapping the bigger Network to the output layer to predict one-day ahead i.e. 288 intervals\n",
    "    x = Dense(dense2, activation=\"relu\")(combinedInput)\n",
    "    x = Dense(288, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[mlp.input, lstm.input], outputs=x)\n",
    "    \n",
    "    # Compiling the model with Mean Absolute Error as loss function and Adam as optimizer\n",
    "    model.compile(loss=loss, optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will try 39 different models with differen parameters, changing 1 thing at a time\n",
    "# Aim is to find the best working parameters\n",
    "\n",
    "\n",
    "models = []\n",
    "\n",
    "#############################################\n",
    "#\n",
    "# Big Model\n",
    "#\n",
    "#############################################\n",
    "\n",
    "# Different Activations\n",
    "models.append(fine_tune(128, 200, 200, 64, 'sigmoid', \n",
    "             0.1, 'mse', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(128, 200, 200, 64, 'relu', \n",
    "             0.1, 'mse', mlp_shape, lstm_shape))\n",
    "\n",
    "# Different Dropouts #\n",
    "models.append(fine_tune(128, 200, 200, 64, 'sigmoid', \n",
    "             0.2, 'mse', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(128, 200, 200, 64, 'relu', \n",
    "             0.2, 'mse', mlp_shape, lstm_shape))\n",
    "\n",
    "# Different loss Functions\n",
    "models.append(fine_tune(128, 200, 200, 64, 'sigmoid', \n",
    "             0.1, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(128, 200, 200, 64, 'sigmoid', \n",
    "             0.2, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(128, 200, 200, 64, 'relu', \n",
    "             0.1, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(128, 200, 200, 64, 'relu', \n",
    "             0.2, 'mae', mlp_shape, lstm_shape))\n",
    "\n",
    "#############################################\n",
    "#\n",
    "# MEDIUM MOEL\n",
    "#\n",
    "#############################################\n",
    "\n",
    "# Different Activations\n",
    "models.append(fine_tune(100, 128, 128, 50, 'sigmoid', \n",
    "             0.1, 'mse', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(100, 128, 128, 50, 'relu', \n",
    "             0.1, 'mse', mlp_shape, lstm_shape))\n",
    "\n",
    "# Different Dropouts #\n",
    "models.append(fine_tune(100, 128, 128, 50, 'sigmoid', \n",
    "             0.2, 'mse', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(100, 128, 128, 50, 'relu', \n",
    "             0.2, 'mse', mlp_shape, lstm_shape))\n",
    "\n",
    "# Different loss Functions\n",
    "models.append(fine_tune(100, 128, 128, 50, 'sigmoid', \n",
    "             0.1, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(100, 128, 128, 50, 'sigmoid', \n",
    "             0.2, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(100, 128, 128, 50, 'relu', \n",
    "             0.1, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(100, 128, 128, 50, 'relu', \n",
    "             0.2, 'mae', mlp_shape, lstm_shape))\n",
    "\n",
    "#############################################\n",
    "#\n",
    "# SMALL MODEL\n",
    "#\n",
    "#############################################\n",
    "\n",
    "# Different Activations\n",
    "models.append(fine_tune(64, 128, 64, 32, 'sigmoid', \n",
    "             0.1, 'mse', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 128, 64, 32, 'relu', \n",
    "             0.1, 'mse', mlp_shape, lstm_shape))\n",
    "\n",
    "# Different Dropouts #\n",
    "models.append(fine_tune(64, 128, 64, 32, 'sigmoid', \n",
    "             0.2, 'mse', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 128, 64, 32, 'relu', \n",
    "             0.2, 'mse', mlp_shape, lstm_shape))\n",
    "\n",
    "# Different loss Functions\n",
    "models.append(fine_tune(64, 128, 64, 32, 'sigmoid', \n",
    "             0.1, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 128, 64, 32, 'sigmoid', \n",
    "             0.2, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 128, 64, 32, 'relu', \n",
    "             0.1, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 128, 64, 32, 'relu', \n",
    "             0.2, 'mae', mlp_shape, lstm_shape))\n",
    "\n",
    "#############################################\n",
    "#\n",
    "# CURRENT MODEL\n",
    "#\n",
    "#############################################\n",
    "\n",
    "# Different Activations\n",
    "models.append(fine_tune(64, 64, 64, 32, 'sigmoid', \n",
    "             0.1, 'mse', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 64, 64, 32, 'relu', \n",
    "             0.1, 'mse', mlp_shape, lstm_shape))\n",
    "\n",
    "# Different Dropouts #\n",
    "models.append(fine_tune(64, 64, 64, 32, 'sigmoid', \n",
    "             0.2, 'mse', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 64, 64, 32, 'relu', \n",
    "             0.2, 'mse', mlp_shape, lstm_shape))\n",
    "\n",
    "# Different loss Functions\n",
    "models.append(fine_tune(64, 64, 64, 32, 'sigmoid', \n",
    "             0.1, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 64, 64, 32, 'sigmoid', \n",
    "             0.2, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 64, 64, 32, 'relu', \n",
    "             0.1, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 64, 64, 32, 'relu', \n",
    "             0.2, 'mae', mlp_shape, lstm_shape))\n",
    "\n",
    "#############################################\n",
    "#\n",
    "# VARIATION OF CURRENT\n",
    "#\n",
    "#############################################\n",
    "\n",
    "# Different Activations\n",
    "models.append(fine_tune(64, 32, 32, 32, 'sigmoid', \n",
    "             0.1, 'mse', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 32, 32, 32, 'relu', \n",
    "             0.1, 'mse', mlp_shape, lstm_shape))\n",
    "\n",
    "# Different Dropouts #\n",
    "models.append(fine_tune(64, 32, 32, 32, 'sigmoid', \n",
    "             0.2, 'mse', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 32, 32, 32, 'relu', \n",
    "             0.2, 'mse', mlp_shape, lstm_shape))\n",
    "\n",
    "# Different loss Functions\n",
    "models.append(fine_tune(64, 32, 32, 32, 'sigmoid', \n",
    "             0.1, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 32, 32, 32, 'sigmoid', \n",
    "             0.2, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 32, 32, 32, 'relu', \n",
    "             0.1, 'mae', mlp_shape, lstm_shape))\n",
    "models.append(fine_tune(64, 32, 32, 32, 'relu', \n",
    "             0.2, 'mae', mlp_shape, lstm_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    results, hist = cm.train_predict_evaluate(model, features_train1, features_train, labels_train, \n",
    "                                       features_test1, features_test, X_test, X_test.index, label_scaler,\n",
    "                                       32, 160);\n",
    "    print(\"Model {}\".format(i))\n",
    "    cm.quantify_performance(results)\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "################ TRAIN ON ALL THE DATA ###################\n",
    "################ USING MODEL 27(BEST) ###################\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = cm.create_mlp(mlp_shape, 64, 32)\n",
    "lstm = cm.create_conv_lstm(lstm_shape, 128, 64, 32, 'relu', 0.1)\n",
    "\n",
    "combinedInput = concatenate([mlp.output, lstm.output])\n",
    "\n",
    "x = Dense(32, activation=\"relu\")(combinedInput)\n",
    "x = Dense(288, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=[mlp.input, lstm.input], outputs=x)\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, hist = cm.train_predict_evaluate(model, features_train1, features_train, labels_train, \n",
    "                                       features_test1, features_test, X_test, X_test.index, label_scaler,\n",
    "                                       32, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.plot_chart(results[\"2019-01-01\":\"2019-01-25\"], legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.plot_chart(pd.DataFrame(hist.history), xlab='Training Epoch', ylab='Mean Squared Error', title='Training and Validation Error over the Course of Training', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.quantify_performance(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input prices into optimizer\n",
    "import optimizer_module as om\n",
    "numDays = 100 # Number of days to run model\n",
    "start = 0 # Starting time interval from price data\n",
    "bStorage0 = 0 # Starting battery charge\n",
    "\n",
    "predPrices = results.iloc[start:start+(numDays*288)][\"prediction\"].tolist()\n",
    "realPrices = results.iloc[start:start+(numDays*288)][\"true values\"].tolist()\n",
    "outputResults = 1\n",
    "outputActions = 1\n",
    "\n",
    "print(\"Optimizer results for real prices\")\n",
    "realNxtAction, realNxtBatCharge, realActions = om.optimize(realPrices, bStorage0, outputResults, outputActions)\n",
    "\n",
    "print(\"\\nOptimizer results for predicted prices\\nProfit is incorrect as it is calculating predicted profit not actual profit\")\n",
    "predNxtAction, predNxtBatCharge, predActions = om.optimize(predPrices, bStorage0, outputResults, outputActions)\n",
    "\n",
    "maxProfit = sum([realActions[i]*realPrices[i]/12 for i in range(numDays*288)])\n",
    "actualProfit = sum([predActions[i]*realPrices[i]/12 for i in range(numDays*288)])\n",
    "\n",
    "print(\"\\n----------RESULTS----------\")\n",
    "print(\"Max profit possible: $%.4g\" % (maxProfit))\n",
    "print(\"Actual profit: $%.4g -> %.4g%% of max profit possible\" % (actualProfit,actualProfit/maxProfit*100))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
